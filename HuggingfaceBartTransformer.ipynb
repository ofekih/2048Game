{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HuggingfaceBartTransformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ofekih/2048Game/blob/master/HuggingfaceBartTransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBxHpAcPrKuo"
      },
      "source": [
        "# Transformer Training and Evaluating Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQr4V79WrVxc"
      },
      "source": [
        "This was designed to be run directly from Google Colab, your mileage may vary if run from elsewhere."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOaukFEerTxm"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssWkriPVrW3-"
      },
      "source": [
        "Skip this step if running locally.\n",
        "\n",
        "\n",
        "This code will use a model and some helper files that are stored in a GitHub repository. The repository will be cloned temporarily."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKkXnnTJbM6d"
      },
      "source": [
        "!git clone https://github.com/mayaschwarz/cs175--lfric-to-Albert.git git/\n",
        "%cd git/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNL12PzsrfuM"
      },
      "source": [
        "# Install once, then restart the runtime\n",
        "!pip install -r requirements.txt > /dev/null 2> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC__FVDQriBl"
      },
      "source": [
        "#### IMPORTANT\n",
        "\n",
        "Be sure to restart (**not** factory reset) the runtime after running the above cell. Then continue running the code below. To do this, click on Runtime -> Restart runtime. You do not need to do the `pip install`s again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugeSqMa1rwkS"
      },
      "source": [
        "%cd git/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE_a9VHqr5Fg"
      },
      "source": [
        "Import necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNj9cx216vqT"
      },
      "source": [
        "from datasets import load_dataset\n",
        "import datasets as metric_datasets\n",
        "from transformers import (\n",
        "    BartForConditionalGeneration, BartTokenizer,\n",
        "    Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        ")\n",
        "\n",
        "import torch\n",
        "meteor_metric = metric_datasets.load_metric('meteor')\n",
        "\n",
        "from src.data_manager import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwpsSs-ef3Cc"
      },
      "source": [
        "## Preparing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-JRw7LqRRGf"
      },
      "source": [
        "First, pick which bible versions should be the source and target version, where the model will learn to translate verses from the source version to the target version. This works for any of the 7 Modern English versions in our corpora, namely: `t_asv`, `t_bbe`, `t_dby`, `t_kjv`, `t_wbt`, `t_web`, and `t_ylt`.\n",
        "\n",
        "Then, decide how you wish to normalize your text.\n",
        "\n",
        "Finally, decide what your maximum number of words should be per verse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9k0-g_-e3BU"
      },
      "source": [
        "source_version, target_version = 't_kjv', 't_bbe'\n",
        "lowercase_text = False\n",
        "no_punctuation = False\n",
        "MAX_NUM_WORDS = 40"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvH4UchNBOrl"
      },
      "source": [
        "# Set up the datasets\n",
        "versions = get_bible_versions_by_file_name([source_version, target_version])\n",
        "\n",
        "preprocess_operations = [preprocess_filter_num_words(MAX_NUM_WORDS)]\n",
        "\n",
        "if lowercase_text:\n",
        "    preprocess_operations.append(preprocess_lowercase())\n",
        "\n",
        "if no_punctuation:\n",
        "    preprocess_operations.append(preprocess_remove_punctuation(preserve_periods = False))\n",
        "\n",
        "datasets = create_datasets(versions, 0.85, preprocess_operations = preprocess_operations, write_files = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ru98nUwhXtw"
      },
      "source": [
        "def zip_data(dataset: dict) -> [dict]:\n",
        "    \"\"\"\n",
        "    Returns a zipped list containing both the source and target versions for each verse in the dataset.\n",
        "\n",
        "    Arguments:\n",
        "        dataset: {dict} -- a single dataset returned by create_datasets or load_datasets\n",
        "\n",
        "    Returns:\n",
        "        [\n",
        "            {\n",
        "                't_bbe': 'and pilate gave his decision for their desire to be put into effect',\n",
        "                't_kjv': 'and pilate gave sentence that it should be as they required'\n",
        "            },\n",
        "            {\n",
        "                't_bbe': 'for these are the days of punishment in which all the things in the writings will be put into effect',\n",
        "                't_kjv': 'for these be the days of vengeance that all things which are written may be fulfilled'\n",
        "            },\n",
        "            ...\n",
        "        ]\n",
        "    \"\"\"\n",
        "    zipped_data = list()\n",
        "    for source_line, target_line in zip(dataset[source_version], dataset[target_version]):\n",
        "        zipped_data.append({\n",
        "            source_version: source_line,\n",
        "            target_version: target_line,\n",
        "        })\n",
        "\n",
        "    return zipped_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6Enm48ygcVl"
      },
      "source": [
        "training_data = zip_data(datasets['training'])\n",
        "validation_data = zip_data(datasets['validation'])\n",
        "test_data = zip_data(datasets['test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gEO3-OOX5PS"
      },
      "source": [
        "# Take a look at some of the data to see if it looks as expected\n",
        "test_data[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SHzkn_rHdCB"
      },
      "source": [
        "def data_collator(features: list) -> 'batch':\n",
        "    \"\"\"\n",
        "    Creates a sequence-to-sequence training batch, as used by HuggingFace's Seq2SeqTrainer.\n",
        "    \"\"\"\n",
        "    labels = [f[target_version] for f in features]\n",
        "    inputs = [f[source_version] for f in features]\n",
        "\n",
        "    batch = tokenizer.prepare_seq2seq_batch(\n",
        "        src_texts = inputs,\n",
        "        src_lang = 'en_XX',\n",
        "        tgt_lang = 'en_XX',\n",
        "        tgt_texts = labels,\n",
        "        max_length = MAX_NUM_WORDS,\n",
        "        max_target_length = MAX_NUM_WORDS + 5\n",
        "    )\n",
        "\n",
        "    for k in batch:\n",
        "        batch[k] = torch.tensor(batch[k])\n",
        "\n",
        "    return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njsk5l47eay9"
      },
      "source": [
        "# Load a pre-trained sequence-to-sequence model. This model will be fine-tuned.\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5SUfYJ2rvXs"
      },
      "source": [
        "# Load a pre-trained sequence-to-sequence tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C19xqP_h9gj"
      },
      "source": [
        "## Initiating model and trainer for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqKgdnkL_O1h"
      },
      "source": [
        "# defining training related arguments\n",
        "args = Seq2SeqTrainingArguments(output_dir = 'bible-bart',\n",
        "    do_train = True,\n",
        "    do_eval = True,\n",
        "    load_best_model_at_end = True,\n",
        "    evaluation_strategy = 'epoch',\n",
        "    per_device_train_batch_size = 16,\n",
        "    per_device_eval_batch_size = 16,\n",
        "    learning_rate = 1e-5,\n",
        "    num_train_epochs = 3,\n",
        "    logging_dir = '/logs',\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnPMKdcZ72rI"
      },
      "source": [
        "# defining trainer using ðŸ¤—\n",
        "trainer = Seq2SeqTrainer(model = model,\n",
        "    args = args,\n",
        "    data_collator = data_collator,\n",
        "    train_dataset = training_data,\n",
        "    eval_dataset = validation_data,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gKlnTQphwb7"
      },
      "source": [
        "## Training time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvgIO3vO72xE"
      },
      "source": [
        "# I will take hours to train this model upon this table.\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir26BgPB72z5"
      },
      "source": [
        "model_path = f'bart-{source_version[2:]}-to-{target_version[2:]}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn-D4usZtX3E"
      },
      "source": [
        "# Saved with timestamp to avoid overwriting previous save\n",
        "import time\n",
        "model_path = f'{model_path}-{int(time.time())}'\n",
        "trainer.save_model(model_path)\n",
        "print(f'Saved model at: {model_path}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiO2XdD0sQdZ"
      },
      "source": [
        "#### IMPORTANT\n",
        "\n",
        "The model was saved in a temporary location. It will be deleted. If you want to retain the results of your model, either change the model_path to your own Google Drive, or download the model using the Google Colaboratory file system."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyyMqgXisx7g"
      },
      "source": [
        "# Load the model on the same device\n",
        "model = BartForConditionalGeneration.from_pretrained(model_path, max_length = 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LppooIonhYCH"
      },
      "source": [
        "# Inference time\n",
        "Let's load the model from hub and use it for inference using ðŸ¤— pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT4g-cJsKpJO"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(f'translation_{source_version[2:]}_to_{target_version[2:]}', model = model, tokenizer = tokenizer)\n",
        "\n",
        "def translate(text: str) -> str:\n",
        "    # translates a single string\n",
        "    return translator(text, return_text = True)[0]['translation_text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5hN7XDXKpLl"
      },
      "source": [
        "# And let us see how our model performeth.\n",
        "translate(\"And the LORD God called unto Adam, and said unto him, Where art thou?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZZWdIu7UqKU"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udfJ3b59UfwK"
      },
      "source": [
        "Now, let's evaluate our model's performance using METEOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzIPGMxRb0B0"
      },
      "source": [
        "def translate_all(translator, verses: [str], num_verses: int = 100) -> [str]:\n",
        "    return [verse['translation_text'] for verse in translator(verses[:num_verses], return_text = True)]\n",
        "\n",
        "def compute_meteor_metric(predictions: [str], references: [str]) -> float:\n",
        "    meteor_metric.add_batch(predictions = predictions, references = references)\n",
        "    return meteor_metric.compute()['meteor']\n",
        "\n",
        "def compute_meteor_metric_easy(translator, num_verses: int) -> float:\n",
        "    predictions = translate_all(translator, datasets['test'][source_version], num_verses)\n",
        "    references = datasets['test'][target_version][:num_verses]\n",
        "    return compute_meteor_metric(predictions, references)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyqkjWiiJMRD"
      },
      "source": [
        "print(f'METEOR score = ~{compute_meteor_metric_easy(translator, 100):.4f}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}